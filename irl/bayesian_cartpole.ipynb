{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that we're converging faster and to the correct posterior\n",
    "# The posterior results in comparisons between the most dense rewards and least dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../simulated_fqi/')\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import shap\n",
    "import configargparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import scipy\n",
    "from environments import Gridworld\n",
    "from models.agents import NFQAgent\n",
    "from models.networks import NFQNetwork, ContrastiveNFQNetwork\n",
    "from util import get_logger, close_logger, load_models, make_reproducible, save_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from train import fqi\n",
    "import tqdm\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "from environments import CartPoleRegulatorEnv\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from irl_gridworld import find_feature_expectations, plot_reward, norm, find_valid_actions, generate_rollout, generate_policy_rollout, runLinearFQI, l2_norm\n",
    "from multiprocessing import Pool\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "plt.rcParams.update({'font.size': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original reward\n",
    "* Parameterized by x_success_range and theta_success_range\n",
    "* The reward has to be minimized for success\n",
    "* Do we want to recover these parameters? I'm not sure if it'll guarantee that we can observe the same behavior. Or the tie is less certain as it is in Gridworld. Also this has nothing to do with improving the Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cart_reward(x_success_range, theta_success_range):\n",
    "    x_threshold = 2.4\n",
    "    theta_threshold_radians = math.pi / 2\n",
    "    cart_pos = [i/10 for i in range(-30, 30)]\n",
    "    angles = [i/10 for i in range(-20, 25, 2)]\n",
    "    reward_matrix = np.zeros((len(angles), len(cart_pos)))\n",
    "    for i, pos in enumerate(cart_pos):\n",
    "        for j, ang in enumerate(angles):\n",
    "            indicator_pos_fg = 0\n",
    "            indicator_ang_fg = 0\n",
    "            indicator_pos_bg = 0\n",
    "            indicator_ang_bg = 0\n",
    "            # In a forbidden state\n",
    "            if (pos < -x_threshold\n",
    "                or pos > x_threshold\n",
    "                or ang < -theta_threshold_radians\n",
    "                or ang > theta_threshold_radians):\n",
    "                reward = 1\n",
    "            # In success range\n",
    "            elif (-x_success_range < pos < x_success_range\n",
    "                and -theta_success_range < ang < theta_success_range):\n",
    "                reward = 0\n",
    "            # Accumulating cost\n",
    "            else:\n",
    "                reward = 0.2\n",
    "            reward_matrix[j,i] = reward\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    ax = sns.heatmap(reward_matrix, xticklabels=cart_pos, yticklabels=angles)\n",
    "    plt.xlabel(\"Cart Position\")\n",
    "    plt.ylabel(\"Pole Angle (radians)\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Cartpole Reward: x_success=\"+str(x_success_range) + \" theta_success:\" + str(theta_success_range))\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_policy_rollouts(x_success, theta_success, init_experience=200, rollout_length=100, epoch=500, verbose=False, nns=10):\n",
    "    is_contrastive=False\n",
    "    train_env_max_steps=100\n",
    "    eval_env_max_steps=3000\n",
    "    discount=0.95\n",
    "    increment_experience=0\n",
    "    hint_to_goal=0\n",
    "    if verbose:\n",
    "        evaluations=5\n",
    "    else:\n",
    "        evaluations=0\n",
    "    rollouts = []\n",
    "    for n in range(nns):\n",
    "        train_env = CartPoleRegulatorEnv(group=0,masscart=1.0,mode=\"train\",  x_success_range=x_success,\n",
    "            theta_success_range=theta_success)\n",
    "        eval_env = CartPoleRegulatorEnv(group=0, masscart=1.0, mode='eval', x_success_range=x_success,\n",
    "            theta_success_range=theta_success)\n",
    "        logger = get_logger()\n",
    "\n",
    "        # Setup agent\n",
    "        nfq_net = ContrastiveNFQNetwork(\n",
    "            state_dim=train_env.state_dim, is_contrastive=is_contrastive\n",
    "        )\n",
    "        optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "        nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "        # NFQ Main loop\n",
    "        # A set of transition samples denoted as D\n",
    "        bg_rollouts = []\n",
    "        total_cost = 0\n",
    "        if init_experience > 0:\n",
    "            for _ in range(init_experience):\n",
    "                rollout_bg, episode_cost = train_env.generate_rollout(\n",
    "                    None, render=False, group=0\n",
    "                )\n",
    "                bg_rollouts.extend(rollout_bg)\n",
    "                total_cost += episode_cost\n",
    "        all_rollouts = bg_rollouts.copy()\n",
    "\n",
    "        bg_rollouts_test = []\n",
    "        if init_experience > 0:\n",
    "            for _ in range(init_experience):\n",
    "                rollout_bg, episode_cost = eval_env.generate_rollout(\n",
    "                    None, render=False, group=0\n",
    "                )\n",
    "                bg_rollouts_test.extend(rollout_bg)\n",
    "        all_rollouts_test = bg_rollouts_test.copy()\n",
    "\n",
    "        bg_success_queue = [0] * 3\n",
    "        for kk, ep in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "            state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(\n",
    "                all_rollouts\n",
    "            )\n",
    "\n",
    "            if not nfq_net.freeze_shared:\n",
    "                loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "            (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate(eval_env, render=False)\n",
    "            bg_success_queue = bg_success_queue[1:]\n",
    "            bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "\n",
    "            if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "                printed_bg = True\n",
    "                nfq_net.freeze_shared = True\n",
    "                if verbose:\n",
    "                    print(\"FREEZING SHARED\")\n",
    "                break\n",
    "\n",
    "        eval_env.step_number = 0\n",
    "        eval_env.max_steps = 1000\n",
    "        performance_bg = []\n",
    "        num_steps_bg = []\n",
    "        for it in range(evaluations):\n",
    "            (\n",
    "                eval_episode_length_bg,\n",
    "                eval_success_bg,\n",
    "                eval_episode_cost_bg,\n",
    "            ) = nfq_agent.evaluate(eval_env, False)\n",
    "            if verbose:\n",
    "                print(eval_episode_length_bg, eval_success_bg)\n",
    "            num_steps_bg.append(eval_episode_length_bg)\n",
    "            performance_bg.append(eval_episode_length_bg)\n",
    "            train_env.close()\n",
    "            eval_env.close()\n",
    "        if verbose:\n",
    "            print(\"BG stayed up for steps: \", num_steps_bg)\n",
    "\n",
    "        for _ in range(rollout_length):\n",
    "            rollout, episode_cost = eval_env.generate_rollout(agent=None, render=False, group=0)\n",
    "            #rollout, episode_cost = eval_env.generate_rollout(nfq_agent, render=False, group=0)\n",
    "            rollouts.extend(rollout)\n",
    "\n",
    "    observations = []\n",
    "    for r in rollouts:\n",
    "        observations.append((r[0], r[1], [x_success, theta_success]))\n",
    "\n",
    "    return rollouts, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse all rollouts\n",
    "def demonstration_density(rollouts, reward=\"\", vmax=150):\n",
    "    cart_pos = [i/10 for i in range(-30, 30)]\n",
    "    angles = [i/10 for i in range(-20, 25)]\n",
    "    demonstration_density = np.zeros((len(angles), len(cart_pos)))\n",
    "    for r in rollouts:\n",
    "        state = r[0]\n",
    "        x = np.round(state[0], 1)\n",
    "        theta = np.round(state[2], 1)\n",
    "\n",
    "        x_ind = cart_pos.index(x)\n",
    "        theta_ind = angles.index(theta)\n",
    "\n",
    "        demonstration_density[theta_ind, x_ind] += 1\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    ax = sns.heatmap(demonstration_density, xticklabels=cart_pos, yticklabels=angles, vmax=vmax)\n",
    "    plt.xlabel(\"Cart Position\")\n",
    "    plt.ylabel(\"Pole Angle (radians)\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(\"Demonstration Density with \" + str(len(rollouts)) + \" samples for reward \" + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_pos = [i/10 for i in range(16, 27, 4)]\n",
    "reward_ang = [i/10 for i in range(2, 8, 3)]\n",
    "for i, pos in enumerate(reward_pos):\n",
    "    for j, ang in enumerate(reward_ang):\n",
    "        reward = [pos, ang]\n",
    "        print(\"Reward: \", reward)\n",
    "        rollouts, b = generate_policy_rollouts(pos, ang, init_experience=200, rollout_length=50, nns=10)\n",
    "        demonstration_density(rollouts, reward, vmax=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional density estimates\n",
    "* Distance_r = absolute distance between the theta and x success ranges\n",
    "* Distance_points = absolute distance between the x position and the theta angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correct set of hyparameters\n",
    "all_distances = []\n",
    "for s in observations:\n",
    "    for s_prime in observations:\n",
    "        r = s[2]\n",
    "        r_prime = s_prime[2]\n",
    "        all_distances.append(distance_r(r, r_prime))\n",
    "sns.distplot(all_distances)\n",
    "print(\"var distance: \", np.std(all_distances)*np.std(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_distances = []\n",
    "for o in observations:\n",
    "    for o_prime in observations:\n",
    "        all_distances.append(distance_points(o, o_prime))\n",
    "sns.distplot(all_distances)\n",
    "print(\"var distance: \", np.std(all_distances)*np.std(all_distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_r(r, r_prime):\n",
    "    h_prime = 0.196 # Proportional to standard deviation of all reward function distances (or variance)\n",
    "    dist_pos = np.absolute(r[0] - r_prime[0])\n",
    "    dist_ang = np.absolute(r[1] - r_prime[1])\n",
    "    dist = dist_pos + dist_ang\n",
    "    return np.exp(-(np.power(dist, 2)/(2*h_prime)))\n",
    "\n",
    "def distance_points(p1, p2):\n",
    "    h=0.147 # Proportional to standard deviation of all distances (or variance)\n",
    "    # Removing the action part\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    state_2 = [p2[0][0], p2[0][2]]\n",
    "    dist = scipy.spatial.distance.euclidean(p1[0], p2[0]) #+ scipy.spatial.distance.euclidean(p1[1], p2[1])\n",
    "    return np.exp(-np.power(dist, 2)/(2*h))\n",
    "\n",
    "def distance_rewards(r_k, observations):\n",
    "    sum_diff = 0\n",
    "    for sample in observations:\n",
    "        r = sample[2]\n",
    "        sum_diff += distance_r(r_k, r)\n",
    "    return sum_diff\n",
    "\n",
    "def conditional_dist(s_i, dataset, reward):\n",
    "    sum = 0\n",
    "    dist_rewards = distance_rewards(reward, dataset)\n",
    "    sum_weights = 0\n",
    "    count_p = 0\n",
    "    for s_j in dataset:\n",
    "        weight = distance_r(reward, s_j[2]) / dist_rewards\n",
    "        dist = distance_points(s_i, s_j)\n",
    "        est = dist * weight \n",
    "        sum += est\n",
    "#     h = 0.003\n",
    "#     h_prime = 0.05\n",
    "#     sum /= np.sqrt(np.power(2*np.pi, 2) * h) * np.sqrt(np.power(2*np.pi, 2) * h_prime)\n",
    "    return sum\n",
    "\n",
    "def estimate_expert_prior(r_k, behavior_opt, observations):\n",
    "    post = 0\n",
    "    dist_rewards = distance_rewards(r_k, observations)\n",
    "    for s_i in behavior_opt:\n",
    "        # Sum\n",
    "#         post += np.log(conditional_dist(s_i, behavior_opt, r_k))\n",
    "        sum_si = 0\n",
    "        for s_j in observations:\n",
    "            weight = distance_r(r_k, s_j[2]) / dist_rewards\n",
    "            likelihood = distance_points(s_i, s_j) * weight\n",
    "            sum_si += likelihood\n",
    "#         h = 0.003\n",
    "#         h_prime = 0.05\n",
    "#         sum_si /= np.sqrt(np.power(2*np.pi, 2) * h) * np.sqrt(np.power(2*np.pi, 2) * h_prime)\n",
    "        if sum_si == 0:\n",
    "            post += np.log(0.000000000001)\n",
    "        else:\n",
    "            post += np.log(sum_si)\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(init_experience=50, rollout_length=10, nns=2):\n",
    "    reward_pos = [i/10 for i in range(10, 27, 10)]\n",
    "    reward_ang = [i/10 for i in range(0, 25, 6)]\n",
    "    behavior = []\n",
    "    for i, pos in enumerate(reward_pos):\n",
    "        for j, ang in enumerate(reward_ang):\n",
    "            rollouts, b = generate_policy_rollouts(pos, ang, init_experience=init_experience, rollout_length=rollout_length, nns=nns)\n",
    "            behavior.extend(b)\n",
    "    \n",
    "    return behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/501 [00:00<00:29, 16.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 463/501 [01:24<00:06,  5.51it/s]\n",
      "100%|██████████| 501/501 [00:44<00:00, 11.32it/s]\n",
      "100%|██████████| 501/501 [00:40<00:00, 12.27it/s]\n",
      "100%|██████████| 501/501 [00:43<00:00, 11.53it/s]\n",
      "100%|██████████| 501/501 [01:42<00:00,  4.87it/s]\n",
      "100%|██████████| 501/501 [01:10<00:00,  7.06it/s]\n",
      "100%|██████████| 501/501 [00:34<00:00, 14.33it/s]\n",
      "100%|██████████| 501/501 [01:39<00:00,  5.02it/s]\n",
      "100%|██████████| 501/501 [00:31<00:00, 15.84it/s]\n",
      " 80%|███████▉  | 400/501 [01:30<00:22,  4.43it/s]\n",
      "100%|██████████| 501/501 [00:50<00:00,  9.88it/s]\n"
     ]
    }
   ],
   "source": [
    "observations = generate_data(init_experience=200, rollout_length=20, nns=1)\n",
    "rollouts, behavior_opt = generate_policy_rollouts(1.3, 12*2 * math.pi / 360, init_experience=200, rollout_length=20, nns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"training_data.txt\", \"w\")\n",
    "for element in observations:\n",
    "    state = np.array2string(element[0], precision=5, separator=',')\n",
    "    action = str(element[1])\n",
    "    reward = str(element[2]) #np.array2string(element[2], precision=5, separator=',')\n",
    "    textfile.write(state + \" \" + action + \" \" + reward + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile = open(\"behavior_opt_13_12.txt\", \"w\")\n",
    "for element in behavior_opt:\n",
    "    state = np.array2string(element[0], precision=5, separator=',')\n",
    "    action = str(element[1])\n",
    "    reward = str(element[2]) #np.array2string(element[2], precision=5, separator=',')\n",
    "    textfile.write(state + \" \" + action + \" \" + reward + \"\\n\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = [1.3, 12*2 * math.pi / 360]\n",
    "cart_pos = [i/10 for i in range(-30, 30)]\n",
    "angles = [i/10 for i in range(-20, 25)]\n",
    "heatmap_conditional = np.zeros((len(angles), len(cart_pos)))\n",
    "for i, pos in enumerate(cart_pos):\n",
    "    for j, ang in enumerate(angles):\n",
    "        state = [pos, ang]\n",
    "        c_est = conditional_dist(state, observations, reward)\n",
    "        heatmap_conditional[j, i] += c_est\n",
    "fig = plt.figure(figsize=(18, 7))\n",
    "plt.xlabel(\"Cart position\")\n",
    "plt.ylabel(\"Pole angle\")\n",
    "ax = sns.heatmap(heatmap_conditional, xticklabels=cart_pos, yticklabels=angles)\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [5:10:34<5:54:40, 2364.54s/it]"
     ]
    }
   ],
   "source": [
    "reward_pos = [i/10 for i in range(10, 27)]\n",
    "reward_ang = [i/10 for i in range(0, 25, 2)]\n",
    "reward = [1.3, 12*2 * math.pi / 360]\n",
    "\n",
    "heatmap_posterior = np.zeros((len(reward_ang), len(reward_pos)))\n",
    "for i, pos in enumerate(tqdm.tqdm(reward_pos)):\n",
    "    for j, ang in enumerate(reward_ang):\n",
    "        r_k = [pos, ang]\n",
    "        post = estimate_expert_prior(r_k, behavior_opt, observations)\n",
    "        heatmap_posterior[j, i] = post\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(5)\n",
    "ax = sns.heatmap(heatmap_posterior)\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"X Success Range\")\n",
    "plt.ylabel(\"Angle Success Range\")\n",
    "plt.title(\"Expert posterior, true reward=\" + str(reward))\n",
    "ax.set_xticklabels(reward_pos, rotation=90)\n",
    "ax.set_yticklabels(reward_ang, rotation=360)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_data.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "print(str(lines[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts, behavior_opt = generate_policy_rollouts(1.4, 12*2 * math.pi / 360, nns=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstration_density(rollouts, vmax=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_to_states = {}\n",
    "rewards_to_states[str(reward)] = states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('rewards_to_states.json', 'w') as fp:\n",
    "    json.dump(rewards_to_states, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dd(states, percentage='100', reward=\"[1.4, 0.0]\", vmax=200):\n",
    "    cart_pos = [i/10 for i in range(-30, 30)]\n",
    "    angles = [i/10 for i in range(-20, 25)]\n",
    "    demonstration_density = np.zeros((len(angles), len(cart_pos)))\n",
    "    for state in states:\n",
    "        x = np.round(state[0], 1)\n",
    "        theta = np.round(state[2], 1)\n",
    "\n",
    "        x_ind = cart_pos.index(x)\n",
    "        theta_ind = angles.index(theta)\n",
    "\n",
    "        demonstration_density[theta_ind, x_ind] += 1\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    ax = sns.heatmap(demonstration_density, xticklabels=cart_pos, yticklabels=angles, vmax=vmax)\n",
    "    plt.xlabel(\"Cart Position\")\n",
    "    plt.ylabel(\"Pole Angle (radians)\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(str(percentage) + \" \" + str(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rewards_to_states.json', 'w') as fp:\n",
    "    json.dump(rewards_to_states, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = [3.6, 1.5]\n",
    "rollouts, behavior_opt = generate_policy_rollouts(reward[0], reward[1], nns=15)\n",
    "states = []\n",
    "for b in behavior_opt:\n",
    "    state = b[0].tolist()\n",
    "    states.append(state)\n",
    "rewards_to_states[str(reward)] = states\n",
    "percentages = [i/10 for i in range(1, 11)]\n",
    "states = np.asarray(rewards_to_states[str(reward)])\n",
    "for p in percentages:\n",
    "    idx = np.random.randint(len(states), size=int(p*len(states)))\n",
    "    s_p = states[idx, :]\n",
    "    dd(s_p, p, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research [~/.conda/envs/research/]",
   "language": "python",
   "name": "conda_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
