{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent vs. ascent\n",
    "# Initialization strategies?\n",
    "# Linear programming problem\n",
    "# Removed action completely from the network in Linear. I think we can't do this completely for q-value fn. \n",
    "# Ask Barbara what class of fns for Q-value\n",
    "# Simulated openai environments, contrastive NN, non-linear, normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../simulated_fqi/')\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import shap\n",
    "import configargparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from environments import Gridworld\n",
    "from models.agents import NFQAgent\n",
    "from models.networks import NFQNetwork, ContrastiveNFQNetwork\n",
    "from util import get_logger, close_logger, load_models, make_reproducible, save_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from train import fqi\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from irl_gridworld import find_feature_expectations, plot_reward, norm, find_valid_actions, generate_rollout, generate_policy_rollout, runLinearFQI, l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQI with Neural Network\n",
    "* It doesn't learn true reward\n",
    "* It only predicts left or up\n",
    "* Gradients for fg and shared are the same. This doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFQI(verbose=True, epoch=100, init_experience=100, evaluations=5, reward_weights_shared=np.asarray([3, 3]),\n",
    "          reward_weights_fg = np.asarray([-6, 0]), run_fg=True, run_bg=True):\n",
    "    # Setup environment\n",
    "    if run_bg:\n",
    "        train_env_bg = Gridworld(group=0, shared_weights=reward_weights_shared, target_state=[3, 3])\n",
    "        eval_env_bg = Gridworld(group=0, shared_weights=reward_weights_shared, target_state=[3, 3])\n",
    "    if run_fg:\n",
    "        train_env_fg = Gridworld(group=1, shared_weights=reward_weights_shared, fg_weights=reward_weights_fg, target_state=[0, 3])\n",
    "        eval_env_fg = Gridworld(group=1, shared_weights=reward_weights_shared, fg_weights=reward_weights_fg, target_state=[0, 3])\n",
    "    \n",
    "    # NFQ Main loop\n",
    "    bg_rollouts = []\n",
    "    fg_rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            if run_bg:\n",
    "                rollout_bg, episode_cost = train_env_bg.generate_rollout(None, render=False, group=0)\n",
    "                bg_rollouts.extend(rollout_bg)\n",
    "            if run_fg:\n",
    "                rollout_fg, episode_cost = train_env_fg.generate_rollout(None, render=False, group=1)\n",
    "                fg_rollouts.extend(rollout_fg)\n",
    "    if run_bg:\n",
    "        all_rollouts = bg_rollouts\n",
    "    if run_fg:\n",
    "        all_rollouts = fg_rollouts\n",
    "    if run_bg and run_fg:\n",
    "        bg_rollouts.extend(fg_rollouts)\n",
    "        all_rollouts = bg_rollouts.copy()\n",
    "\n",
    "    bg_rollouts_test = []\n",
    "    fg_rollouts_test = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            if run_bg:\n",
    "                rollout_bg, episode_cost = eval_env_bg.generate_rollout(None, render=False, group=0)\n",
    "                bg_rollouts_test.extend(rollout_bg)\n",
    "            if run_fg:\n",
    "                rollout_fg, episode_cost = eval_env_fg.generate_rollout(None, render=False, group=1)\n",
    "                fg_rollouts_test.extend(rollout_fg)\n",
    "    if run_bg:\n",
    "        all_rollouts_test = bg_rollouts_test\n",
    "    if run_fg:\n",
    "        all_rollouts_test = fg_rollouts_test\n",
    "    if run_bg and run_fg:\n",
    "        bg_rollouts_test.extend(fg_rollouts)\n",
    "        all_rollouts_test = bg_rollouts_test.copy()\n",
    "    \n",
    "    # Setup agent\n",
    "    nfq_net = ContrastiveNFQNetwork(state_dim=2, is_contrastive=False)\n",
    "\n",
    "    optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "    nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "    bg_success_queue = [0] * 3\n",
    "    fg_success_queue = [0] * 3\n",
    "    epochs_fg = 0\n",
    "    eval_fg = 0\n",
    "    for k, epoch in enumerate(tqdm.tqdm(range(epoch + 1))):\n",
    "        state_b, target_q_values, groups = nfq_agent.generate_pattern_set(\n",
    "            all_rollouts, reward_weights_shared=reward_weights_shared, reward_weights_fg=reward_weights_fg, use_weights=False\n",
    "        )\n",
    "        \n",
    "        if not nfq_net.freeze_shared:\n",
    "            loss = nfq_agent.train((state_b, target_q_values, groups))\n",
    "\n",
    "        eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "        if nfq_net.freeze_shared:\n",
    "            eval_fg += 1\n",
    "            if eval_fg > 50:\n",
    "                loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "        if run_bg:\n",
    "            (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate(eval_env_bg, render=False)\n",
    "        if run_fg:\n",
    "            (eval_episode_length_fg,eval_success_fg,eval_episode_cost_fg) = nfq_agent.evaluate(eval_env_fg, render=False)\n",
    "        \n",
    "        if run_bg:\n",
    "            bg_success_queue = bg_success_queue[1:]\n",
    "            bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "        if run_fg:\n",
    "            fg_success_queue = fg_success_queue[1:]\n",
    "            fg_success_queue.append(1 if eval_success_fg else 0)\n",
    "\n",
    "        printed_bg = False\n",
    "        printed_fg = False\n",
    "\n",
    "        if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "            if epochs_fg == 0:\n",
    "                epochs_fg = epoch\n",
    "            printed_bg = True\n",
    "            nfq_net.freeze_shared = True\n",
    "            if verbose:\n",
    "                print(\"FREEZING SHARED\")\n",
    "                if not run_fg:\n",
    "                    break\n",
    "            for param in nfq_net.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in nfq_net.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            optimizer = optim.Adam(\n",
    "                itertools.chain(\n",
    "                    nfq_net.layers_fg.parameters(),\n",
    "                    nfq_net.layers_last_fg.parameters(),\n",
    "                ),\n",
    "                lr=1e-1,\n",
    "            )\n",
    "            nfq_agent._optimizer = optimizer\n",
    "        if run_fg:\n",
    "            if sum(fg_success_queue) == 3:\n",
    "                printed_fg = True\n",
    "                break\n",
    "\n",
    "    for k in range(evaluations):\n",
    "        if run_bg:\n",
    "            (eval_episode_length_bg,eval_success_bg,eval_episode_cost_bg) = nfq_agent.evaluate(eval_env_bg, False)\n",
    "            if verbose:\n",
    "                print(\"Success: \", eval_success_bg)\n",
    "            train_env_bg.close()\n",
    "            eval_env_bg.close()\n",
    "        if run_fg:\n",
    "            (eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg) = nfq_agent.evaluate(eval_env_fg, False)\n",
    "            if verbose:\n",
    "                print(\"Success: \", eval_success_fg)\n",
    "            train_env_fg.close()\n",
    "            eval_env_fg.close()\n",
    "    if run_fg:\n",
    "        print(\"Fg trained after \" + str(epochs_fg) + \" epochs\")\n",
    "    \n",
    "    print(\"Generating policy rollout\")\n",
    "    bg_rollouts = []\n",
    "    fg_rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _, _ in enumerate(tqdm.tqdm(range(int(init_experience/2)))):\n",
    "            if run_bg:\n",
    "                rollout_bg, episode_cost = train_env_bg.generate_rollout(\n",
    "                    nfq_agent, render=False, group=0, random_start = False\n",
    "                )\n",
    "                bg_rollouts.extend(rollout_bg)\n",
    "            if run_fg:\n",
    "                rollout_fg, episode_cost = train_env_fg.generate_rollout(\n",
    "                    nfq_agent, render=False, group=1, random_start = False\n",
    "                )\n",
    "                fg_rollouts.extend(rollout_fg)\n",
    "    \n",
    "    if run_bg:\n",
    "        policy_rollouts = bg_rollouts\n",
    "    if run_fg:\n",
    "        policy_rollouts = fg_rollouts\n",
    "    if run_bg and run_bg:\n",
    "        bg_rollouts.extend(fg_rollouts)\n",
    "        policy_rollouts = bg_rollouts.copy()\n",
    "    \n",
    "    return policy_rollouts, nfq_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200; learning_rate=0.2; init_w_shared = [0.5]*2; init_w_fg = [0.5]*2\n",
    "w_shared = init_w_shared\n",
    "w_fg = init_w_fg\n",
    "\n",
    "muB_shared = None\n",
    "muB_fg = None\n",
    "\n",
    "diff_shared = []\n",
    "diff_fg = []\n",
    "\n",
    "true_weights_bg = [0.5, 0.5]\n",
    "true_weights_fg = [-2., 0.5]\n",
    "print(\"Generating optimal behavior\")\n",
    "behavior_rollout_bg, opt_agent_bg = runFQI(run_fg=False, run_bg=True, reward_weights_shared=true_weights_bg)\n",
    "behavior_rollout_fg, opt_agent_fg = runFQI(run_fg=True, run_bg=False, reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg)\n",
    "\n",
    "muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "    policy_rollout_bg, agent_pi_bg = runFQI(run_fg=False, run_bg=True, reward_weights_shared=w_shared, reward_weights_fg=None)\n",
    "    policy_rollout_fg, agent_pi_fg = runFQI(run_fg=True, run_bg=False, reward_weights_shared=w_shared, reward_weights_fg=w_fg)\n",
    "\n",
    "    #print('Evaluate feature expectations for pi')\n",
    "    # Generate rollout with this policy, thens do feature expectations\n",
    "    mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "    mu_fg = find_feature_expectations(policy_rollout_fg, shared=False)\n",
    "    print(\"Shared feature expectations: \", mu_shared)\n",
    "    print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "    #print('Gradient update for new w')\n",
    "    grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "    grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "    print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "    w_shared_old = w_shared\n",
    "    # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "    w_shared += learning_rate * grad_shared\n",
    "    w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "    w_fg_old = w_fg\n",
    "    # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "    w_fg += learning_rate * grad_fg\n",
    "    w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "    # Plot difference in feature expectations vs. weight values\n",
    "    # Axes are weight values and each tile is difference in feature expectations from behavior\n",
    "    # Lowest for weights that we want\n",
    "    diff_shared.append(mean_absolute_error(w_shared_old, w_shared))\n",
    "    diff_fg.append(mean_absolute_error(w_fg_old, w_fg))\n",
    "\n",
    "plt.plot(diff_shared)\n",
    "plt.plot(diff_fg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQI with linear Q-value function\n",
    "* Doesn't approximate target Q-values like the NFQ Net does\n",
    "* But it can learn a linear reward function, as expected\n",
    "* The policy rollouts are also pretty successful\n",
    "* Need to generate separate policies to approximate different rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=200; learning_rate=1.; init_w_shared = [0.5]*2; init_w_fg = [0.5]*2\n",
    "w_shared = init_w_shared\n",
    "w_fg = init_w_fg\n",
    "\n",
    "muB_shared = None\n",
    "muB_fg = None\n",
    "\n",
    "diff_shared = []\n",
    "diff_fg = []\n",
    "\n",
    "true_weights_bg = [0.5, 0.5]\n",
    "true_weights_fg = [-1., 0]\n",
    "print(\"Generating behavior with true weights shared=\" + str(true_weights_bg) + \" true_weights_fg=\" + str(true_weights_fg))\n",
    "behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "\n",
    "muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False, g=1)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "    policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=None, dataset='bg')\n",
    "    policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=w_fg, dataset='fg')\n",
    "\n",
    "    #print('Evaluate feature expectations for pi')\n",
    "    # Generate rollout with this policy, then do feature expectations\n",
    "    mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "    mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "    print(\"Shared feature expectations: \", mu_shared)\n",
    "    print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "    #print('Gradient update for new w')\n",
    "    grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "    grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "    print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "    w_shared_old = w_shared\n",
    "    # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "    w_shared += learning_rate * grad_shared\n",
    "    w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "    w_fg_old = w_fg\n",
    "    # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "    w_fg += learning_rate * grad_fg\n",
    "    # Don't normalize here: Normalizing here doesn't really matter\n",
    "    w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "    diff_shared.append(mean_absolute_error(w_shared_old, w_shared))\n",
    "    diff_fg.append(mean_absolute_error(w_fg_old, w_fg))\n",
    "\n",
    "plt.plot(diff_shared)\n",
    "plt.plot(diff_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot, 4boxes (2 for each weights), fg --> plot sum, draw from gaussian (mean 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward(n=4, title='Shared True Reward', reward_weights_shared=None, reward_weights_fg=None, agent=None, file=None):\n",
    "    reward_matrix = np.zeros((n, n))\n",
    "    positions = [i for i in range(n)]\n",
    "    for i, x in enumerate(range(n)):\n",
    "        for j, y in enumerate(range(n)):\n",
    "            if reward_weights_shared is not None and reward_weights_fg is not None:\n",
    "                reward = np.dot([x, y], np.add(reward_weights_shared, reward_weights_fg))\n",
    "            elif reward_weights_shared is not None:\n",
    "                reward = np.dot([x, y], reward_weights_shared)\n",
    "            elif agent is not None:\n",
    "                reward = agent.predict([[x, y]])\n",
    "            reward_matrix[j,i] = reward\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    ax = sns.heatmap(reward_matrix, xticklabels=positions, yticklabels=positions)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(title, fontsize=20)\n",
    "    if file is not None:\n",
    "        plt.savefig(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"True weights bg: \" + str(true_weights_bg) + \" Recovered weights bg: \" + str(w_shared))\n",
    "print(\"True weights fg: \" + str(np.add(true_weights_bg,true_weights_fg)) + \" Recovered weights fg: \" + str(np.add(w_shared, w_fg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward(title=\"Background true reward\", reward_weights_shared=[0, 1], file='g1_true_reward.png')\n",
    "plot_reward(title=\"Foreground true reward\", reward_weights_shared=[0, 1], reward_weights_fg = [1, 1], file='g2_true_reward.png')\n",
    "\n",
    "#plot_reward(title='Opt agent shared reward', agent=opt_agent_bg)\n",
    "#plot_reward(title='Background reward predicted by CIRL', reward_weights_shared=w_shared, agent=None, file='g1_pred_reward.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_reward(title='Opt agent fg reward', agent=opt_agent_fg)\n",
    "plot_reward(title='Foreground reward predicted by CIRL', reward_weights_shared=w_shared, reward_weights_fg=w_fg, agent=None, file='g2_pred_reward.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping out the space of losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(muB_shared, muB_fg, weights_bg=None, weights_fg=None, true_weights_bg=[3, 3], true_weights_fg=[-6, 0], fg=True):\n",
    "    if fg:\n",
    "        policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=weights_bg, reward_weights_fg=weights_fg, dataset='fg')\n",
    "        mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "        diff = np.linalg.norm(muB_fg-mu_fg)\n",
    "    else:\n",
    "        policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=weights_bg, reward_weights_fg=None, dataset='bg')\n",
    "        mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "        diff = np.linalg.norm(muB_shared-mu_shared)\n",
    "\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False, g=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_matrix_fg = np.zeros((15, 15))\n",
    "positions = [i/100 for i in range(-300, 300, 40)]\n",
    "for i, x in enumerate(positions):\n",
    "    for j, y in enumerate(positions):\n",
    "        diff_matrix_fg[j,i] = calculate_loss(muB_shared=muB_shared, muB_fg=muB_fg, weights_bg=[0.5, 0.5], weights_fg=[x, y], fg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_matrix_bg = np.zeros((15, 15))\n",
    "positions = [i/100 for i in range(-300, 300, 40)]\n",
    "for i, x in enumerate(positions):\n",
    "    for j, y in enumerate(positions):\n",
    "        diff_matrix_bg[j,i] = calculate_loss(muB_shared=muB_shared, muB_fg=muB_fg, weights_bg=[x, y], fg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "ax = sns.heatmap(diff_matrix_bg, xticklabels=positions, yticklabels=positions)\n",
    "plt.xlabel(\"Bg Weight Elt 0\")\n",
    "plt.ylabel(\"Bg Weight Elt 1\")\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Difference in feature expectations for background weights\", fontsize=20)\n",
    "plt.savefig(\"bg_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "ax = sns.heatmap(diff_matrix_fg, xticklabels=positions, yticklabels=positions)\n",
    "plt.xlabel(\"FG Weight Elt 0\")\n",
    "plt.ylabel(\"FG Weight Elt 1\")\n",
    "ax.invert_yaxis()\n",
    "plt.title(\"Difference in feature expectations for foreground weights\", fontsize=20)\n",
    "plt.savefig(\"fg_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporary Conclusions\n",
    "* CIRL recovers the closest reward weights to the initialized weights that are consistent with the observations\n",
    "* Dependent on initialization. We learn bg weights really well since the initialization basically gives it away. \n",
    "The fg weights are a little harder to learn, but we're pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of weights experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridworld_irl(epochs=200, learning_rate=1., init_w_shared = [0.5]*2, init_w_fg = [0.5]*2, verbose=False):\n",
    "    w_shared = init_w_shared\n",
    "    w_fg = init_w_fg\n",
    "\n",
    "    muB_shared = None\n",
    "    muB_fg = None\n",
    "\n",
    "    true_weights_bg = [0.5, 0.5]\n",
    "    true_weights_fg = [-6., 0]\n",
    "    if verbose:\n",
    "        print(\"Generating behavior with true weights shared=\" + str(true_weights_bg) + \" true_weights_fg=\" + str(true_weights_fg))\n",
    "    behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "    behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "\n",
    "    muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "    muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if verbose:\n",
    "            print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "        policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=None, dataset='bg')\n",
    "        policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=w_fg, dataset='fg')\n",
    "\n",
    "        #print('Evaluate feature expectations for pi')\n",
    "        # Generate rollout with this policy, then do feature expectations\n",
    "        mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "        mu_fg = find_feature_expectations(policy_rollout_fg, shared=False)\n",
    "        if verbose:\n",
    "            print(\"Shared feature expectations: \", mu_shared)\n",
    "            print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "        #print('Gradient update for new w')\n",
    "        grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "        grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "        if verbose:\n",
    "            print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "        w_shared_old = w_shared\n",
    "        # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "        w_shared += learning_rate * grad_shared\n",
    "        w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "        w_fg_old = w_fg\n",
    "        # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "        w_fg += learning_rate * grad_fg\n",
    "        # Don't normalize here: Normalizing here doesn't really matter\n",
    "        w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "    \n",
    "    return norm(w_fg + w_shared), w_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_one = []\n",
    "fg_two = []\n",
    "shared_one = []\n",
    "shared_two = []\n",
    "for _, i in enumerate(tqdm.tqdm(range(20))):\n",
    "    w_shared = np.random.normal(size=2)\n",
    "    w_fg = np.random.normal(size=2)\n",
    "    \n",
    "    w_shared = w_shared / norm(w_shared)\n",
    "    w_fg = w_fg / norm(w_fg)\n",
    "    \n",
    "    learned_fg, learned_shared = gridworld_irl(init_w_shared=w_shared, init_w_fg=w_fg)\n",
    "    \n",
    "    fg_one.append(learned_fg[0])\n",
    "    fg_two.append(learned_fg[1])\n",
    "    shared_one.append(learned_shared[0])\n",
    "    shared_two.append(learned_shared[1])\n",
    "    print(\"learned fg: \" + str(learned_fg) + \" learned bg: \" + str(learned_shared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'Fg Weight 0': fg_one, 'Fg Weight 1': fg_two, 'Shared Weight 0': shared_one, 'Shared Weight 1': shared_two}\n",
    "df = pd.DataFrame(data=d)\n",
    "ax = sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df))\n",
    "plt.title(\"Gridworld CIRL weight variation: original fg=[-6., 0], original shared=[0.5, 0.5]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing CIRL to IRL for two groups and IRL for both groups combined\n",
    "* IRL for two groups without the shared gradient performs just as well as CIRL...\n",
    "* Adding in the prior distribution on Beta --> this forces the weights to be positive? \n",
    "* And do we also still need normalization? We still need normalization, otherwise the weights explode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize B ~ N(0, 1/lambda^2)\n",
    "def cirl(true_weights_bg=[0., 1], true_weights_fg=[0.5, 0.5], verbose=False):\n",
    "    epochs=1000; learning_rate=1.\n",
    "    mu, sigma = 0, 0.5 \n",
    "    init_w_shared = np.random.normal(mu, sigma, 2)\n",
    "    init_w_shared = init_w_shared/np.sum(np.abs(init_w_shared))\n",
    "    init_w_fg = np.random.normal(mu, sigma, 2)\n",
    "    init_w_fg = init_w_fg/np.sum(np.abs(init_w_fg))\n",
    "    \n",
    "    lambd = 0.01\n",
    "    w_shared = init_w_shared\n",
    "    w_fg = init_w_fg\n",
    "\n",
    "    muB_shared = None\n",
    "    muB_fg = None\n",
    "\n",
    "    diff_shared = []\n",
    "    diff_fg = []\n",
    "    \n",
    "    # Demonstrations\n",
    "    behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "    behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "    \n",
    "    # Demonstration feature expections\n",
    "    muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "    muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False, g=1)\n",
    "\n",
    "    for _, i in enumerate(tqdm.tqdm(range(epochs))):\n",
    "        if verbose:\n",
    "            print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "        policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=None, dataset='bg')\n",
    "        policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=w_shared, reward_weights_fg=w_fg, dataset='fg')\n",
    "\n",
    "        mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "        mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "        \n",
    "        # Also minimize the l2 norm of the current set of weights scaled by lambda --> variance of prior (0.01)\n",
    "        grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg) + lambd * l2_norm(mu_shared)\n",
    "        grad_fg = norm(muB_fg) - norm(mu_fg) + lambd* l2_norm(mu_fg)\n",
    "\n",
    "        w_shared_old = w_shared\n",
    "        w_shared += learning_rate * grad_shared\n",
    "        w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "        w_fg_old = w_fg\n",
    "        # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "        w_fg += learning_rate * grad_fg\n",
    "        w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "        diff_shared.append(mean_absolute_error(w_shared_old, w_shared))\n",
    "        diff_fg.append(mean_absolute_error(w_fg_old, w_fg))\n",
    "    \n",
    "    plot_reward(title='CIRL background predicted reward', reward_weights_shared=w_shared, agent=None, file='cirl_g1_pred_reward.png')\n",
    "    plot_reward(title='CIRL foreground predicted reward', reward_weights_shared=w_shared, reward_weights_fg=w_fg, agent=None, file='cirl_g2_pred_reward.png')\n",
    "    plt.plot(diff_shared)\n",
    "    plt.plot(diff_fg)\n",
    "    return w_shared, w_fg\n",
    "    \n",
    "def irl(true_weights_bg=[0.5, 0.5], true_weights_fg=[-1., 0]):\n",
    "    epochs=200; learning_rate=1.; init_w_shared = [0.5]*2; init_w_fg = [0.5]*2\n",
    "    w_bg = init_w_shared\n",
    "    w_fg = init_w_fg\n",
    "\n",
    "    muB_bg = None\n",
    "    muB_fg = None\n",
    "\n",
    "\n",
    "    print(\"Generating behavior with true weights bg=\" + str(true_weights_bg) + \" true_weights_fg=\" + str(true_weights_fg))\n",
    "    behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "    behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "\n",
    "    muB_bg = find_feature_expectations(behavior_rollout_bg, shared=False, g=0)\n",
    "    muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False, g=1)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('Epoch', i, '- Train pi with current w_bg='+str(w_bg) + \" w_fg=\", w_fg )\n",
    "        policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=w_bg, reward_weights_fg=None, dataset='bg')\n",
    "        policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=w_bg, reward_weights_fg=w_fg, dataset='fg')\n",
    "\n",
    "        mu_bg = find_feature_expectations(policy_rollout_bg, shared=False, g=0)\n",
    "        mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "\n",
    "        grad_bg = norm(muB_bg) - norm(mu_bg)\n",
    "        grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "\n",
    "        w_bg_old = w_bg\n",
    "        w_bg += learning_rate * grad_bg\n",
    "        w_bg = w_bg/np.sum(np.abs(w_bg))\n",
    "\n",
    "        w_fg_old = w_fg\n",
    "        # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "        w_fg += learning_rate * grad_fg\n",
    "        # Don't normalize here: Normalizing here doesn't really matter\n",
    "        w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "    \n",
    "    plot_reward(title='CIRL background predicted reward', reward_weights_shared=w_bg, agent=None, file='irl_g1_pred_reward.png')\n",
    "    plot_reward(title='CIRL foreground predicted reward', reward_weights_shared=w_bg, reward_weights_fg=w_fg, agent=None, file='irl_g2_pred_reward.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:24<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shared:  [0.43312245 0.56687755]\n",
      "W fg:  [-0.28083409  0.71916591]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_shared, w_fg = cirl()\n",
    "print(\"W shared: \", w_shared)\n",
    "print(\"W fg: \", w_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = math.sqrt(variance)\n",
    "x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "plt.plot(x, stats.norm.pdf(x, mu, sigma))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prior uniform so that MAP = MLE. \n",
    "# P(D | theta fg, theta bg) = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "# P(theta) = lambd * l2_norm(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-14-e3a4308e1060>\u001b[0m(54)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     53 \u001b[0;31m    \u001b[0;31m# Calculate which direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 54 \u001b[0;31m    \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_old\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mf_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     55 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> muB_shared\n",
      "array([1.83950617, 1.7962963 ])\n",
      "ipdb> norm(muB_shared)\n",
      "array([0.50594228, 0.49405772])\n"
     ]
    }
   ],
   "source": [
    "# Metropolis Hasting\n",
    "true_weights_bg=[0., 1]; true_weights_fg=[0.5, 0.5]\n",
    "\n",
    "# Demonstrations\n",
    "behavior_rollout_bg, opt_agent_bg = runLinearFQI(dataset='bg', reward_weights_shared=true_weights_bg, behavior=True)\n",
    "behavior_rollout_fg, opt_agent_fg = runLinearFQI(dataset='fg', reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, behavior=True)\n",
    "\n",
    "# Demonstration feature expections\n",
    "muB_shared = find_feature_expectations(behavior_rollout_bg, shared=True)\n",
    "muB_fg = find_feature_expectations(behavior_rollout_fg, shared=False, g=1)\n",
    "\n",
    "mu, sigma = 0, 0.5 \n",
    "prev_w_shared = np.random.normal(mu, sigma, 2)\n",
    "prev_w_shared = prev_w_shared/np.sum(np.abs(prev_w_shared))\n",
    "prev_w_fg = np.random.normal(mu, sigma, 2)\n",
    "prev_w_fg = prev_w_fg/np.sum(np.abs(prev_w_fg))\n",
    "\n",
    "# Previous muFg, muShared\n",
    "policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=prev_w_shared, reward_weights_fg=None, dataset='bg')\n",
    "policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=prev_w_shared, reward_weights_fg=prev_w_fg, dataset='fg')\n",
    "\n",
    "prev_mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "prev_mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "\n",
    "delta = 0.01\n",
    "# Keep track of all of thetas, that's the full posterior\n",
    "\n",
    "for i in range(1000):\n",
    "    # g function is a gaussian with prev sample as mean\n",
    "    new_w_shared_zero = np.random.normal(prev_w_shared[0], delta)\n",
    "    new_w_shared_one = np.random.normal(prev_w_shared[1], delta)\n",
    "    \n",
    "    new_w_fg_zero = np.random.normal(prev_w_fg[0], delta)\n",
    "    new_w_fg_one = np.random.normal(prev_w_fg[1], delta)\n",
    "    \n",
    "    new_w_shared = [new_w_shared_zero, new_w_shared_one]\n",
    "    new_w_fg = [new_w_fg_zero, new_w_fg_one]\n",
    "    \n",
    "    # Forward RL with the new weights\n",
    "    policy_rollout_bg, agent_pi_bg = runLinearFQI(reward_weights_shared=new_w_shared, reward_weights_fg=None, dataset='bg')\n",
    "    policy_rollout_fg, agent_pi_fg = runLinearFQI(reward_weights_shared=new_w_shared, reward_weights_fg=new_w_fg, dataset='fg')\n",
    "    \n",
    "    # Calculate muFg and muShared\n",
    "    mu_shared = find_feature_expectations(policy_rollout_bg, shared=True)\n",
    "    mu_fg = find_feature_expectations(policy_rollout_fg, shared=False, g=1)\n",
    "    \n",
    "    # F : not true likelihood , gibbs posterior\n",
    "    # e ^ -L(theta) * e^-l2 norm(theta)\n",
    "    \n",
    "    f_old = np.exp(norm(muB_shared) - norm(prev_mu_shared) + norm(muB_fg) - norm(prev_mu_fg))\n",
    "    f_new = np.exp(norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg))\n",
    "    \n",
    "    import ipdb; ipdb.set_trace()\n",
    "    \n",
    "    # Calculate which direction\n",
    "    alpha = f_old/f_new\n",
    "    \n",
    "    # Calculate accept or pass on each element\n",
    "    u = np.random.uniform((0, 1))\n",
    "    if u < alpha[0]:\n",
    "        # accept\n",
    "        prev_w_shared_zero = new_w_shared[0]\n",
    "        prev_w_shared_one = new_w_shared[1]\n",
    "        prev_w_fg_zero = new_w_fg[0]\n",
    "        prev_w\n",
    "    \n",
    "    else:\n",
    "        # Don't accept\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research [~/.conda/envs/research/]",
   "language": "python",
   "name": "conda_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
