{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../simulated_fqi/')\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import shap\n",
    "import configargparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from environments import CartPoleRegulatorEnv\n",
    "from models.networks import NFQNetwork, ContrastiveNFQNetwork\n",
    "from models.agents import NFQAgent\n",
    "from util import get_logger, close_logger, load_models, make_reproducible, save_models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from train import fqi\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from irl_gridworld import find_feature_expectations, plot_reward, norm, find_valid_actions, generate_rollout, generate_policy_rollout, runLinearFQI\n",
    "from train import fqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying the state representation using a phi function\n",
    "* [a, b, c, d] --> [a, b, c, d, e (indicator if cart in success range), f (indicator if pole in success range)]\n",
    "* There's no combination of reward that allows the success range to appear as a box (?), maybe I'm plotting something wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'shared_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bcd3049f5001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleRegulatorEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasscart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshared_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfg_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_rollouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrollout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mall_rollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'shared_weights'"
     ]
    }
   ],
   "source": [
    "train_env = CartPoleRegulatorEnv(group=0,masscart=1.0,mode=\"train\",shared_weights=[0.2, 0.1, 0.3, 0.4], fg_weights=[0.2, 0.2, 0.3, 0.3])\n",
    "all_rollouts = []\n",
    "for _ in range(10):\n",
    "    rollout, episode_cost = train_env.generate_rollout(None, render=False, group=0)\n",
    "    all_rollouts.extend(rollout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [r[2] for r in all_rollouts]\n",
    "sns.distplot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole with Linear Reward\n",
    "* Just define two reward functions (smaller allowable range) (bg has narrower range)\n",
    "* The state is the indicators (1, 1) or (pos, angle) or (-4.8, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_feature_expectations(dataset):\n",
    "    features_pos_fg = []\n",
    "    features_ang_fg = []\n",
    "    features_pos_bg = []\n",
    "    features_ang_bg = []\n",
    "    for state in dataset:\n",
    "        s = state[0]\n",
    "        _, _, _, _, indicator_pos_fg, indicator_pos_bg, indicator_ang_fg, indicator_ang_bg = s\n",
    "        features_pos_fg.append(indicator_pos_fg)\n",
    "        features_pos_bg.append(indicator_pos_bg)\n",
    "        features_ang_fg.append(indicator_ang_fg)\n",
    "        features_ang_bg.append(indicator_ang_bg)\n",
    "\n",
    "    avg_pos_fg = np.mean(features_pos_fg)\n",
    "    avg_pos_bg = np.mean(features_pos_bg)\n",
    "    avg_ang_fg = np.mean(features_ang_fg)\n",
    "    avg_ang_bg = np.mean(features_ang_bg)\n",
    "    \n",
    "    return [avg_pos_fg, avg_pos_bg, avg_ang_fg, avg_ang_bg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFQI(init_experience=400, verbose=True, group=0, shared_weights=[0.2, 0.1, 0.3, 0.4], fg_weights=[0.2, 0.2, 0.3, 0.3]):\n",
    "    if group == 0:\n",
    "        force_left = 0\n",
    "    else:\n",
    "        force_left = 1\n",
    "    train_env = CartPoleRegulatorEnv(group=0,masscart=1.0,mode=\"train\",shared_weights=shared_weights, fg_weights=fg_weights)\n",
    "    eval_env = CartPoleRegulatorEnv(group=0, masscart=1.0, mode='eval',shared_weights=shared_weights, fg_weights=fg_weights)\n",
    "    all_rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout, episode_cost = train_env.generate_rollout(None, render=False, group=group)\n",
    "            all_rollouts.extend(rollout)\n",
    "    \n",
    "    reg = LinearRegression()\n",
    "    gamma = 0.95\n",
    "    state_b, action_b, cost_b, next_state_b, done_b, group_b = zip(*all_rollouts)\n",
    "    done_b = torch.FloatTensor(done_b)\n",
    "    cost_b = torch.FloatTensor(cost_b)\n",
    "    next_state_b = np.asarray(next_state_b)\n",
    "    next_state_b = next_state_b[:, 4:]\n",
    "    reg = LinearRegression().fit(next_state_b, cost_b)\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout, episode_cost = eval_env.generate_rollout(reg, render=False, group=0)\n",
    "            rollouts.extend(rollout)\n",
    "\n",
    "    policy_rollouts = rollouts\n",
    "    return policy_rollouts, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30; learning_rate=1; init_w_shared = [0.5]*4; init_w_fg = [0.5]*4\n",
    "w_shared = init_w_shared\n",
    "w_fg = init_w_fg\n",
    "\n",
    "muB_shared = None\n",
    "muB_fg = None\n",
    "\n",
    "diff_shared = []\n",
    "diff_fg = []\n",
    "\n",
    "true_weights_bg = [0.2, 0.1, 0.3, 0.4]\n",
    "true_weights_fg = [0.2, 0.2, 0.3, 0.3]\n",
    "print(\"Generating behavior with true weights shared=\" + str(true_weights_bg) + \" true_weights_fg=\" + str(true_weights_fg))\n",
    "behavior_rollout_bg, opt_agent_bg = runFQI(group=0, shared_weights=true_weights_bg)\n",
    "behavior_rollout_fg, opt_agent_fg = runFQI(group=1, shared_weights=true_weights_bg, fg_weights=true_weights_fg)\n",
    "\n",
    "muB_shared = cart_feature_expectations(behavior_rollout_bg)\n",
    "muB_fg = cart_feature_expectations(behavior_rollout_fg)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "    policy_rollout_bg, agent_pi_bg = runFQI(shared_weights=w_shared, fg_weights=None,group=0)\n",
    "    policy_rollout_fg, agent_pi_fg = runFQI(shared_weights=w_shared, fg_weights=w_fg,group=1)\n",
    "    \n",
    "    #print('Evaluate feature expectations for pi')\n",
    "    # Generate rollout with this policy, then do feature expectations\n",
    "    mu_shared = cart_feature_expectations(policy_rollout_bg)\n",
    "    mu_fg = cart_feature_expectations(policy_rollout_fg)\n",
    "    print(\"Shared feature expectations: \", mu_shared)\n",
    "    print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "    #print('Gradient update for new w')\n",
    "    grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "    grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "    print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "    w_shared_old = w_shared\n",
    "    # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "    w_shared += learning_rate * grad_shared\n",
    "    w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "    w_fg_old = w_fg\n",
    "    # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "    w_fg += learning_rate * grad_fg\n",
    "    # Don't normalize here: Normalizing here doesn't really matter\n",
    "    w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "    diff_shared.append(mean_absolute_error(w_shared_old, w_shared))\n",
    "    diff_fg.append(mean_absolute_error(w_fg_old, w_fg))\n",
    "\n",
    "plt.plot(diff_shared)\n",
    "plt.plot(diff_fg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fg = w_fg + w_shared\n",
    "reward_bg = w_shared\n",
    "print(\"True Reward FG: \" + str(true_weights_fg) + \" Learned Reward Fg: \" + str(reward_fg))\n",
    "print(\"True Reward BG: \" + str(true_weights_bg) + \" Learned Reward Bg: \" + str(reward_bg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the reward over a densely sampled grid of cart position and pole angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_cart(reward_weights_shared=None, reward_weights_fg=None, agent=None, title='Learned Shared Reward'):\n",
    "    x_success_range_fg = 2.4\n",
    "    theta_success_range_fg = 12 * 2 * math.pi / 360\n",
    "    x_success_range_bg = 2.0\n",
    "    theta_success_range_bg = 10 * 2 * math.pi / 360\n",
    "    cart_pos = [i/10 for i in range(-48, 48)]\n",
    "    angles = [i/100 for i in range(-40, 40, 2)]\n",
    "    reward_matrix = np.zeros((len(angles), len(cart_pos)))\n",
    "    for i, pos in enumerate(cart_pos):\n",
    "        for j, ang in enumerate(angles):\n",
    "            indicator_pos_fg = 0\n",
    "            indicator_ang_fg = 0\n",
    "            indicator_pos_bg = 0\n",
    "            indicator_ang_bg = 0\n",
    "            if -x_success_range_fg < pos < x_success_range_fg:\n",
    "                indicator_pos_fg = 1\n",
    "            if -theta_success_range_fg < ang < theta_success_range_fg:\n",
    "                indicator_ang_fg = 1\n",
    "            if -x_success_range_bg < pos < x_success_range_bg:\n",
    "                indicator_pos_bg = 1\n",
    "            if -theta_success_range_bg < ang < theta_success_range_bg:\n",
    "                indicator_ang_bg = 1\n",
    "            state = [indicator_pos_fg, indicator_pos_bg, indicator_ang_fg, indicator_ang_bg]\n",
    "            if reward_weights_shared is not None and reward_weights_fg is not None:\n",
    "                reward = np.dot(state, np.add(reward_weights_shared, reward_weights_fg))\n",
    "            elif reward_weights_shared is not None:\n",
    "                reward = np.dot(state, reward_weights_shared)\n",
    "            elif agent is not None:\n",
    "                reward = agent.predict([state])\n",
    "            reward_matrix[j,i] = reward\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    ax = sns.heatmap(reward_matrix, xticklabels=cart_pos, yticklabels=angles)\n",
    "    plt.xlabel(\"Cart Position\")\n",
    "    plt.ylabel(\"Pole Angle (radians)\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_weights_bg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-92c342071a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_reward_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_weights_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Shared True Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_reward_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Learned Shared Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_reward_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_weights_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_fg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_weights_fg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Fg True Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_reward_cart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_shared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_weights_fg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_fg\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mw_shared\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Learned Fg Reward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_weights_bg' is not defined"
     ]
    }
   ],
   "source": [
    "plot_reward_cart(agent=None, reward_weights_shared=true_weights_bg, title='Shared True Reward')\n",
    "plot_reward_cart(agent=None, reward_weights_shared=w_shared, title='Learned Shared Reward')\n",
    "\n",
    "plot_reward_cart(agent=None, reward_weights_shared=true_weights_bg, reward_weights_fg=true_weights_fg, title='Fg True Reward')\n",
    "plot_reward_cart(agent=None, reward_weights_shared=w_shared, reward_weights_fg=w_fg+w_shared, title=\"Learned Fg Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization vs. returned weights experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpoleFQI(epochs=20, learning_rate=1, init_w_shared = [0.5]*4, init_w_fg = [0.5]*4, verbose=False):\n",
    "    w_shared = init_w_shared\n",
    "    w_fg = init_w_fg\n",
    "\n",
    "    muB_shared = None\n",
    "    muB_fg = None\n",
    "\n",
    "    true_weights_bg = [1, 2, 4, 5]\n",
    "    true_weights_fg = [5, 3, 2, 1]\n",
    "    if verbose:\n",
    "        print(\"Generating behavior with true weights shared=\" + str(true_weights_bg) + \" true_weights_fg=\" + str(true_weights_fg))\n",
    "    behavior_rollout_bg, opt_agent_bg = runFQI(group=0, shared_weights=true_weights_bg)\n",
    "    behavior_rollout_fg, opt_agent_fg = runFQI(group=1, shared_weights=true_weights_bg, fg_weights=true_weights_fg)\n",
    "\n",
    "    muB_shared = cart_feature_expectations(behavior_rollout_bg)\n",
    "    muB_fg = cart_feature_expectations(behavior_rollout_fg)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        if verbose:\n",
    "            print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "        policy_rollout_bg, agent_pi_bg = runFQI(shared_weights=w_shared, fg_weights=None,group=0)\n",
    "        policy_rollout_fg, agent_pi_fg = runFQI(shared_weights=w_shared, fg_weights=w_fg,group=1)\n",
    "\n",
    "        #print('Evaluate feature expectations for pi')\n",
    "        # Generate rollout with this policy, then do feature expectations\n",
    "        mu_shared = cart_feature_expectations(policy_rollout_bg)\n",
    "        mu_fg = cart_feature_expectations(policy_rollout_fg)\n",
    "        if verbose:\n",
    "            print(\"Shared feature expectations: \", mu_shared)\n",
    "            print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "        #print('Gradient update for new w')\n",
    "        grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "        grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "        if verbose:\n",
    "            print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "        w_shared_old = w_shared\n",
    "        # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "        w_shared += learning_rate * grad_shared\n",
    "        w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "        w_fg_old = w_fg\n",
    "        # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "        w_fg += learning_rate * grad_fg\n",
    "        # Don't normalize here: Normalizing here doesn't really matter\n",
    "        w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "    return norm(w_fg + w_shared), w_shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fg_one = []\n",
    "fg_two = []\n",
    "fg_three = []\n",
    "fg_four = []\n",
    "shared_one = []\n",
    "shared_two = []\n",
    "shared_three = []\n",
    "shared_four = []\n",
    "for _, i in enumerate(tqdm.tqdm(range(20))):\n",
    "    w_shared = np.random.normal(size=4)\n",
    "    w_fg = np.random.normal(size=4)\n",
    "    \n",
    "    w_shared = w_shared / norm(w_shared)\n",
    "    w_fg = w_fg / norm(w_fg)\n",
    "    \n",
    "    learned_fg, learned_shared = cartpoleFQI(init_w_shared=w_shared, init_w_fg=w_fg)\n",
    "    \n",
    "    fg_one.append(learned_fg[0])\n",
    "    fg_two.append(learned_fg[1])\n",
    "    fg_three.append(learned_fg[2])\n",
    "    fg_four.append(learned_fg[3])\n",
    "    shared_one.append(learned_shared[0])\n",
    "    shared_two.append(learned_shared[1])\n",
    "    shared_three.append(learned_shared[2])\n",
    "    shared_four.append(learned_shared[3])\n",
    "    print(\"learned fg: \" + str(learned_fg) + \" learned bg: \" + str(learned_shared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {'Fg Weight 0': fg_one, 'Fg Weight 1': fg_two, 'Fg Weight 2': fg_three, 'Fg Weight 3': fg_three,'Shared Weight 0': shared_one, 'Shared Weight 1': shared_two, 'Shared Weight 2': shared_three, 'Shared Weight 3': shared_four}\n",
    "df = pd.DataFrame(data=d)\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df))\n",
    "plt.title(\"Cartpole CIRL weight variation: original fg=[5, 3, 2, 1], original shared=[1, 2, 4, 5]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi function that is a CFQNetwork on Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveNFQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, is_contrastive: bool = True, nonlinearity=nn.Sigmoid):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        LAYER_WIDTH = self.state_dim + 1\n",
    "        self.is_contrastive = is_contrastive\n",
    "        self.freeze_shared = False\n",
    "        self.freeze_fg = False\n",
    "\n",
    "        self.layers_shared = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + 1, LAYER_WIDTH),\n",
    "            nonlinearity(),\n",
    "            nn.Linear(LAYER_WIDTH, LAYER_WIDTH),\n",
    "            nonlinearity(),\n",
    "        )\n",
    "        self.layers_fg = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + 1, LAYER_WIDTH),\n",
    "            nonlinearity(),\n",
    "            nn.Linear(LAYER_WIDTH, LAYER_WIDTH),\n",
    "            nonlinearity(),\n",
    "        )\n",
    "        self.layers_last_shared = nn.Sequential(\n",
    "            nn.Linear(LAYER_WIDTH, 1), nonlinearity()\n",
    "        )\n",
    "        self.layers_last_fg = nn.Sequential(nn.Linear(LAYER_WIDTH, 1), nonlinearity())\n",
    "        self.layers_last = nn.Sequential(nn.Linear(LAYER_WIDTH * 2, 1), nonlinearity())\n",
    "        # Initialize weights to [-0.5, 0.5]\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.uniform_(m.weight, -0.5, 0.5)\n",
    "\n",
    "        def init_weights_fg(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.zeros_(m.weight)\n",
    "\n",
    "        self.layers_shared.apply(init_weights)\n",
    "\n",
    "        # if self.is_contrastive:\n",
    "        self.layers_last_shared.apply(init_weights)\n",
    "        self.layers_fg.apply(init_weights_fg)\n",
    "        self.layers_last_fg.apply(init_weights_fg)\n",
    "        self.layers_last.apply(init_weights)\n",
    "\n",
    "        if is_contrastive:\n",
    "            for param in self.layers_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layers_last_fg.parameters():\n",
    "                param.requires_grad = False\n",
    "        # else:\n",
    "        #    self.layers_last.apply(init_weights)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, group=0) -> torch.Tensor:\n",
    "\n",
    "        x = self.layers_shared(x)\n",
    "        if not self.is_contrastive:\n",
    "            group = 1\n",
    "            \n",
    "        x_shared = self.layers_last_shared(x)\n",
    "\n",
    "        x_fg = self.layers_fg(x)\n",
    "        x_fg = self.layers_last_fg(x_fg)\n",
    "        \n",
    "        return x_shared + x_fg * group\n",
    "\n",
    "    def freeze_shared_layers(self):\n",
    "        for param in self.layers_shared.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.layers_last_shared.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_fg_layers(self):\n",
    "        for param in self.layers_fg.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.layers_last_fg.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_last_layers(self):\n",
    "        for param in self.layers_last_shared.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.layers_last_fg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_last_layers(self):\n",
    "        for param in self.layers_last_shared.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.layers_last_fg.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def assert_correct_layers_frozen(self):\n",
    "\n",
    "        if not self.is_contrastive:\n",
    "            for param in self.layers_fg.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_last_fg.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_shared.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_last_shared.parameters():\n",
    "                assert param.requires_grad == True\n",
    "\n",
    "        elif self.freeze_shared:\n",
    "            for param in self.layers_fg.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_last_fg.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_shared.parameters():\n",
    "                assert param.requires_grad == False\n",
    "            for param in self.layers_last_shared.parameters():\n",
    "                assert param.requires_grad == False\n",
    "        else:\n",
    "\n",
    "            for param in self.layers_fg.parameters():\n",
    "                assert param.requires_grad == False\n",
    "            for param in self.layers_last_fg.parameters():\n",
    "                assert param.requires_grad == False\n",
    "            for param in self.layers_shared.parameters():\n",
    "                assert param.requires_grad == True\n",
    "            for param in self.layers_last_shared.parameters():\n",
    "                assert param.requires_grad == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fqi(verbose=True, is_contrastive=False, epochs=1000, init_experience=200, evaluations=5,force_left=5):\n",
    "\n",
    "    # Setup environment\n",
    "    bg_cart_mass = 1.0\n",
    "    fg_cart_mass = 1.0\n",
    "    train_env_bg = CartPoleRegulatorEnv(group=0,masscart=bg_cart_mass,mode=\"train\",force_left=force_left,is_contrastive=is_contrastive)\n",
    "    train_env_fg = CartPoleRegulatorEnv(group=1,masscart=fg_cart_mass,mode=\"train\",force_left=force_left,is_contrastive=is_contrastive)\n",
    "    eval_env_bg = CartPoleRegulatorEnv(group=0,masscart=bg_cart_mass,mode=\"eval\",force_left=force_left,is_contrastive=is_contrastive)\n",
    "    eval_env_fg = CartPoleRegulatorEnv(group=1,masscart=fg_cart_mass,mode=\"eval\",force_left=force_left,is_contrastive=is_contrastive)\n",
    "    nfq_net = ContrastiveNFQNetwork(state_dim=train_env_bg.state_dim, is_contrastive=is_contrastive)\n",
    "    # Setup agent\n",
    "    if is_contrastive:\n",
    "        optimizer = optim.Adam(\n",
    "            itertools.chain(\n",
    "                nfq_net.layers_shared.parameters(),\n",
    "                nfq_net.layers_last_shared.parameters(),\n",
    "            ),\n",
    "            lr=1e-1,\n",
    "        )\n",
    "    else:\n",
    "        optimizer = optim.Adam(nfq_net.parameters(), lr=1e-1)\n",
    "\n",
    "    nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "    # NFQ Main loop\n",
    "    bg_rollouts = []\n",
    "    fg_rollouts = []\n",
    "    total_cost = 0\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout_bg, episode_cost = train_env_bg.generate_rollout(\n",
    "                None, render=False, group=0\n",
    "            )\n",
    "            rollout_fg, episode_cost = train_env_fg.generate_rollout(\n",
    "                None, render=False, group=1\n",
    "            )\n",
    "            bg_rollouts.extend(rollout_bg)\n",
    "            fg_rollouts.extend(rollout_fg)\n",
    "            total_cost += episode_cost\n",
    "    bg_rollouts.extend(fg_rollouts)\n",
    "    all_rollouts = bg_rollouts.copy()\n",
    "\n",
    "    bg_rollouts_test = []\n",
    "    fg_rollouts_test = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout_bg, episode_cost = eval_env_bg.generate_rollout(\n",
    "                None, render=False, group=0\n",
    "            )\n",
    "            rollout_fg, episode_cost = eval_env_fg.generate_rollout(\n",
    "                None, render=False, group=1\n",
    "            )\n",
    "            bg_rollouts_test.extend(rollout_bg)\n",
    "            fg_rollouts_test.extend(rollout_fg)\n",
    "    bg_rollouts_test.extend(fg_rollouts)\n",
    "    all_rollouts_test = bg_rollouts_test.copy()\n",
    "\n",
    "    state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(\n",
    "        all_rollouts_test\n",
    "    )\n",
    "    X_test = state_action_b\n",
    "    test_groups = groups\n",
    "\n",
    "    bg_success_queue = [0] * 3\n",
    "    fg_success_queue = [0] * 3\n",
    "    epochs_fg = 0\n",
    "    eval_fg = 0\n",
    "    for _, epoch in enumerate(tqdm.tqdm(range(epochs))):\n",
    "\n",
    "        state_action_b, target_q_values, groups = nfq_agent.generate_pattern_set(\n",
    "            all_rollouts\n",
    "        )\n",
    "        X = state_action_b\n",
    "        train_groups = groups\n",
    "\n",
    "        if not nfq_net.freeze_shared:\n",
    "            loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "        eval_episode_length_fg, eval_success_fg, eval_episode_cost_fg = 0, 0, 0\n",
    "        if nfq_net.freeze_shared:\n",
    "            eval_fg += 1\n",
    "\n",
    "            if eval_fg > 50:\n",
    "                loss = nfq_agent.train((state_action_b, target_q_values, groups))\n",
    "\n",
    "        if is_contrastive:\n",
    "            if nfq_net.freeze_shared:\n",
    "                (\n",
    "                    eval_episode_length_fg,\n",
    "                    eval_success_fg,\n",
    "                    eval_episode_cost_fg,\n",
    "                ) = nfq_agent.evaluate(eval_env_fg, render=False)\n",
    "                for param in nfq_net.layers_fg.parameters():\n",
    "                    assert param.requires_grad == True\n",
    "                for param in nfq_net.layers_last_fg.parameters():\n",
    "                    assert param.requires_grad == True\n",
    "                for param in nfq_net.layers_shared.parameters():\n",
    "                    assert param.requires_grad == False\n",
    "                for param in nfq_net.layers_last_shared.parameters():\n",
    "                    assert param.requires_grad == False\n",
    "            else:\n",
    "\n",
    "                for param in nfq_net.layers_fg.parameters():\n",
    "                    assert param.requires_grad == False\n",
    "                for param in nfq_net.layers_last_fg.parameters():\n",
    "                    assert param.requires_grad == False\n",
    "                for param in nfq_net.layers_shared.parameters():\n",
    "                    assert param.requires_grad == True\n",
    "                for param in nfq_net.layers_last_shared.parameters():\n",
    "                    assert param.requires_grad == True\n",
    "                (\n",
    "                    eval_episode_length_bg,\n",
    "                    eval_success_bg,\n",
    "                    eval_episode_cost_bg,\n",
    "                ) = nfq_agent.evaluate(eval_env_bg, render=False)\n",
    "\n",
    "        else:\n",
    "            (\n",
    "                eval_episode_length_bg,\n",
    "                eval_success_bg,\n",
    "                eval_episode_cost_bg,\n",
    "            ) = nfq_agent.evaluate(eval_env_bg, render=False)\n",
    "            (\n",
    "                eval_episode_length_fg,\n",
    "                eval_success_fg,\n",
    "                eval_episode_cost_fg,\n",
    "            ) = nfq_agent.evaluate(eval_env_fg, render=False)\n",
    "\n",
    "        bg_success_queue = bg_success_queue[1:]\n",
    "        bg_success_queue.append(1 if eval_success_bg else 0)\n",
    "\n",
    "        fg_success_queue = fg_success_queue[1:]\n",
    "        fg_success_queue.append(1 if eval_success_fg else 0)\n",
    "\n",
    "        printed_bg = False\n",
    "        printed_fg = False\n",
    "\n",
    "        if sum(bg_success_queue) == 3 and not nfq_net.freeze_shared == True:\n",
    "            if epochs_fg == 0:\n",
    "                epochs_fg = epoch\n",
    "            printed_bg = True\n",
    "            nfq_net.freeze_shared = True\n",
    "            if verbose:\n",
    "                print(\"FREEZING SHARED\")\n",
    "            if is_contrastive:\n",
    "                for param in nfq_net.layers_shared.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in nfq_net.layers_last_shared.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in nfq_net.layers_fg.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in nfq_net.layers_last_fg.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                for param in nfq_net.layers_fg.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in nfq_net.layers_last_fg.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "                optimizer = optim.Adam(\n",
    "                    itertools.chain(\n",
    "                        nfq_net.layers_fg.parameters(),\n",
    "                        nfq_net.layers_last_fg.parameters(),\n",
    "                    ),\n",
    "                    lr=1e-1,\n",
    "                )\n",
    "                nfq_agent._optimizer = optimizer\n",
    "            # break\n",
    "\n",
    "        if sum(fg_success_queue) == 3:\n",
    "            printed_fg = True\n",
    "            break\n",
    "\n",
    "    eval_env_bg.step_number = 0\n",
    "    eval_env_fg.step_number = 0\n",
    "\n",
    "    eval_env_bg.max_steps = 1000\n",
    "    eval_env_fg.max_steps = 1000\n",
    "\n",
    "    performance_fg = []\n",
    "    performance_bg = []\n",
    "    num_steps_bg = []\n",
    "    num_steps_fg = []\n",
    "    total = 0\n",
    "    for it in range(evaluations):\n",
    "        (\n",
    "            eval_episode_length_bg,\n",
    "            eval_success_bg,\n",
    "            eval_episode_cost_bg,\n",
    "        ) = nfq_agent.evaluate(eval_env_bg, False)\n",
    "        if verbose:\n",
    "            print(eval_episode_length_bg, eval_success_bg)\n",
    "        num_steps_bg.append(eval_episode_length_bg)\n",
    "        performance_bg.append(eval_episode_length_bg)\n",
    "        total += 1\n",
    "        train_env_bg.close()\n",
    "        eval_env_bg.close()\n",
    "\n",
    "        (\n",
    "            eval_episode_length_fg,\n",
    "            eval_success_fg,\n",
    "            eval_episode_cost_fg,\n",
    "        ) = nfq_agent.evaluate(eval_env_fg, False)\n",
    "        if verbose:\n",
    "            print(eval_episode_length_fg, eval_success_fg)\n",
    "        num_steps_fg.append(eval_episode_length_fg)\n",
    "        performance_fg.append(eval_episode_length_fg)\n",
    "        total += 1\n",
    "        train_env_fg.close()\n",
    "        eval_env_fg.close()\n",
    "    print(\"Fg trained after \" + str(epochs_fg) + \" epochs\")\n",
    "    print(\"BG stayed up for steps: \", num_steps_bg)\n",
    "    print(\"FG stayed up for steps: \", num_steps_fg)\n",
    "    \n",
    "    rollouts = []\n",
    "    if init_experience > 0:\n",
    "        for _ in range(init_experience):\n",
    "            rollout, episode_cost = eval_env_bg.generate_rollout(nfq_agent, render=False, group=0)\n",
    "            rollouts.extend(rollout)\n",
    "            \n",
    "            rollout, episode_cost = eval_env_fg.generate_rollout(nfq_agent, render=False, group=1)\n",
    "            rollouts.extend(rollout)\n",
    "            \n",
    "    policy_rollouts = rollouts\n",
    "    return policy_rollouts, reg\n",
    "    \n",
    "    return nfq_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix dynamics\n",
    "optAgent, policy_rollout = fqi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20; learning_rate=1; verbose=False\n",
    "\n",
    "if verbose:\n",
    "    print(\"Generating optimal behavior using CFQNetwork\")\n",
    "optAgent = runFQI(group=0, shared_weights=true_weights_bg)\n",
    "\n",
    "muB_shared = cart_feature_expectations(behavior_rollout_bg)\n",
    "muB_fg = cart_feature_expectations(behavior_rollout_fg)\n",
    "\n",
    "for i in range(epochs):\n",
    "    if verbose:\n",
    "        print('Epoch', i, '- Train pi with current w_shared='+str(w_shared) + \" w_fg=\", w_fg )\n",
    "    policy_rollout_bg, agent_pi_bg = runFQI(shared_weights=w_shared, fg_weights=None,group=0)\n",
    "    policy_rollout_fg, agent_pi_fg = runFQI(shared_weights=w_shared, fg_weights=w_fg,group=1)\n",
    "\n",
    "    #print('Evaluate feature expectations for pi')\n",
    "    # Generate rollout with this policy, then do feature expectations\n",
    "    mu_shared = cart_feature_expectations(policy_rollout_bg)\n",
    "    mu_fg = cart_feature_expectations(policy_rollout_fg)\n",
    "    if verbose:\n",
    "        print(\"Shared feature expectations: \", mu_shared)\n",
    "        print(\"Fg feature expectations: \", mu_fg)\n",
    "\n",
    "    #print('Gradient update for new w')\n",
    "    grad_shared = norm(muB_shared) - norm(mu_shared) + norm(muB_fg) - norm(mu_fg)\n",
    "    grad_fg = norm(muB_fg) - norm(mu_fg)\n",
    "    if verbose:\n",
    "        print(\"Grad shared: \" + str(grad_shared) + \" Grad fg: \" + str(grad_fg))\n",
    "\n",
    "    w_shared_old = w_shared\n",
    "    # w_shared += learning_rate*(0.95**i) * grad_shared\n",
    "    w_shared += learning_rate * grad_shared\n",
    "    w_shared = w_shared/np.sum(np.abs(w_shared))\n",
    "\n",
    "    w_fg_old = w_fg\n",
    "    # w_fg += learning_rate*(0.95**i) * grad_fg\n",
    "    w_fg += learning_rate * grad_fg\n",
    "    # Don't normalize here: Normalizing here doesn't really matter\n",
    "    w_fg = w_fg/np.sum(np.abs(w_fg))\n",
    "\n",
    "#return norm(w_fg + w_shared), w_shared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research [~/.conda/envs/research/]",
   "language": "python",
   "name": "conda_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
